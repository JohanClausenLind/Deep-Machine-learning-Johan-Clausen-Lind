{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "DependencyNotInstalled",
     "evalue": "opencv-python package not installed, run `pip install \"gymnasium[other]\"` to get dependencies for atari",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/Documents/johangit/MLVenv/lib/python3.11/site-packages/gymnasium/wrappers/atari_preprocessing.py:97\u001b[0m, in \u001b[0;36mAtariPreprocessing.__init__\u001b[0;34m(self, env, noop_max, frame_skip, screen_size, terminal_on_life_loss, grayscale_obs, grayscale_newaxis, scale_obs)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 97\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcv2\u001b[39;00m  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cv2'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mDependencyNotInstalled\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 305\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;66;03m# Run training if executed directly\u001b[39;00m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 305\u001b[0m     model, metrics \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_dqn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 140\u001b[0m, in \u001b[0;36mtrain_dqn\u001b[0;34m()\u001b[0m\n\u001b[1;32m    137\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(config\u001b[38;5;241m.\u001b[39mcheckpoint_dir, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    139\u001b[0m \u001b[38;5;66;03m# Set up environment\u001b[39;00m\n\u001b[0;32m--> 140\u001b[0m env \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_env\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;66;03m# Create Q-networks\u001b[39;00m\n\u001b[1;32m    143\u001b[0m model \u001b[38;5;241m=\u001b[39m create_q_model(config\u001b[38;5;241m.\u001b[39mnum_actions)\n",
      "Cell \u001b[0;32mIn[4], line 103\u001b[0m, in \u001b[0;36mcreate_env\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m    101\u001b[0m env \u001b[38;5;241m=\u001b[39m gym\u001b[38;5;241m.\u001b[39mmake(config\u001b[38;5;241m.\u001b[39menv_name, render_mode\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mrender_mode)\n\u001b[1;32m    102\u001b[0m \u001b[38;5;66;03m# Apply Atari preprocessing: max pooling, frame skipping, etc.\u001b[39;00m\n\u001b[0;32m--> 103\u001b[0m env \u001b[38;5;241m=\u001b[39m \u001b[43mgym\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrappers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAtariPreprocessing\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m    \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe_skip\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscreen_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m84\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrayscale_obs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscale_obs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[43m    \u001b[49m\u001b[43mterminal_on_life_loss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# This helps with training\u001b[39;49;00m\n\u001b[1;32m    110\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;66;03m# Use our custom frame stacking\u001b[39;00m\n\u001b[1;32m    112\u001b[0m env \u001b[38;5;241m=\u001b[39m CustomFrameStack(env, config\u001b[38;5;241m.\u001b[39mnum_stack)\n",
      "File \u001b[0;32m~/Documents/johangit/MLVenv/lib/python3.11/site-packages/gymnasium/wrappers/atari_preprocessing.py:99\u001b[0m, in \u001b[0;36mAtariPreprocessing.__init__\u001b[0;34m(self, env, noop_max, frame_skip, screen_size, terminal_on_life_loss, grayscale_obs, grayscale_newaxis, scale_obs)\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcv2\u001b[39;00m  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[0;32m---> 99\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m gym\u001b[38;5;241m.\u001b[39merror\u001b[38;5;241m.\u001b[39mDependencyNotInstalled(\n\u001b[1;32m    100\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mopencv-python package not installed, run `pip install \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgymnasium[other]\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m` to get dependencies for atari\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    101\u001b[0m     )\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m frame_skip \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(screen_size, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m screen_size \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(screen_size, \u001b[38;5;28mtuple\u001b[39m)\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(screen_size) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(size, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m size \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m size \u001b[38;5;129;01min\u001b[39;00m screen_size)\n\u001b[1;32m    108\u001b[0m ), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpect the `screen_size` to be positive, actually: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscreen_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mDependencyNotInstalled\u001b[0m: opencv-python package not installed, run `pip install \"gymnasium[other]\"` to get dependencies for atari"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import gymnasium as gym\n",
    "import ale_py\n",
    "from collections import deque\n",
    "import time\n",
    "\n",
    "# Register Atari environments\n",
    "gym.register_envs(ale_py)\n",
    "\n",
    "# Custom frame stacking to replace gymnasium's FrameStack\n",
    "class CustomFrameStack:\n",
    "    def __init__(self, env, num_stack=4):\n",
    "        self.env = env\n",
    "        self.num_stack = num_stack\n",
    "        self.frames = deque(maxlen=num_stack)\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=0, high=255, \n",
    "            shape=(84, 84, num_stack), \n",
    "            dtype=np.uint8\n",
    "        )\n",
    "        self.action_space = env.action_space\n",
    "        \n",
    "    def reset(self, **kwargs):\n",
    "        obs, info = self.env.reset(**kwargs)\n",
    "        for _ in range(self.num_stack):\n",
    "            self.frames.append(obs)\n",
    "        stacked_frames = np.stack(self.frames, axis=-1)\n",
    "        return stacked_frames, info\n",
    "    \n",
    "    def step(self, action):\n",
    "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "        self.frames.append(obs)\n",
    "        stacked_frames = np.stack(self.frames, axis=-1)\n",
    "        return stacked_frames, reward, terminated, truncated, info\n",
    "    \n",
    "    def close(self):\n",
    "        return self.env.close()\n",
    "        \n",
    "    def render(self):\n",
    "        return self.env.render()\n",
    "\n",
    "# Configuration parameters\n",
    "class DQNConfig:\n",
    "    def __init__(self):\n",
    "        # Environment\n",
    "        self.env_name = \"SpaceInvadersNoFrameskip-v4\"\n",
    "        self.render_mode = None  # Set to \"human\" to visualize\n",
    "        self.num_stack = 4\n",
    "        \n",
    "        # Training hyperparameters\n",
    "        self.gamma = 0.99  # Discount factor\n",
    "        self.learning_rate = 0.00025\n",
    "        self.batch_size = 32\n",
    "        self.epsilon_start = 1.0\n",
    "        self.epsilon_min = 0.1\n",
    "        self.epsilon_decay_frames = 1000000  # Frames over which to decay epsilon\n",
    "        self.target_update_freq = 10000  # Update target network every N steps\n",
    "        \n",
    "        # Memory\n",
    "        self.replay_memory_size = 100000  # Reduced from original paper to save memory\n",
    "        self.min_replay_memory_size = 10000  # Start training after this many frames\n",
    "        \n",
    "        # Training limits\n",
    "        self.total_frames = 5000000  # Total frames to train for (reduced from paper's 50M)\n",
    "        self.max_episode_length = 10000  # Max frames per episode\n",
    "        \n",
    "        # Checkpointing\n",
    "        self.save_freq = 50000  # Save model every N frames\n",
    "        self.checkpoint_dir = \"checkpoints\"\n",
    "        \n",
    "        # Game specific\n",
    "        self.num_actions = 6  # Space Invaders has 6 actions\n",
    "        \n",
    "    def epsilon_by_frame(self, frame):\n",
    "        # Linear epsilon decay\n",
    "        epsilon = self.epsilon_start - frame * (self.epsilon_start - self.epsilon_min) / self.epsilon_decay_frames\n",
    "        return max(self.epsilon_min, epsilon)\n",
    "\n",
    "# Deep Q-Network architecture\n",
    "def create_q_model(num_actions):\n",
    "    model = keras.Sequential([\n",
    "        layers.Input(shape=(84, 84, 4)),\n",
    "        layers.Conv2D(32, kernel_size=8, strides=4, activation=\"relu\"),\n",
    "        layers.Conv2D(64, kernel_size=4, strides=2, activation=\"relu\"),\n",
    "        layers.Conv2D(64, kernel_size=3, strides=1, activation=\"relu\"),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(512, activation=\"relu\"),\n",
    "        layers.Dense(num_actions, activation=\"linear\")\n",
    "    ])\n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.00025, clipnorm=1.0),\n",
    "                  loss=keras.losses.Huber())\n",
    "    return model\n",
    "\n",
    "# Create environment with preprocessing\n",
    "def create_env(config):\n",
    "    env = gym.make(config.env_name, render_mode=config.render_mode)\n",
    "    # Apply Atari preprocessing: max pooling, frame skipping, etc.\n",
    "    env = gym.wrappers.AtariPreprocessing(\n",
    "        env,\n",
    "        frame_skip=4,\n",
    "        screen_size=84,\n",
    "        grayscale_obs=True,\n",
    "        scale_obs=False,\n",
    "        terminal_on_life_loss=True  # This helps with training\n",
    "    )\n",
    "    # Use our custom frame stacking\n",
    "    env = CustomFrameStack(env, config.num_stack)\n",
    "    return env\n",
    "\n",
    "# Experience replay memory\n",
    "class ReplayMemory:\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque(maxlen=capacity)\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.memory, batch_size)\n",
    "        states, actions, rewards, next_states, dones = map(np.array, zip(*batch))\n",
    "        return states, actions, rewards, next_states, dones\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "# Main training function\n",
    "def train_dqn():\n",
    "    # Initialize configuration\n",
    "    config = DQNConfig()\n",
    "    \n",
    "    # Create checkpoint directory\n",
    "    os.makedirs(config.checkpoint_dir, exist_ok=True)\n",
    "    \n",
    "    # Set up environment\n",
    "    env = create_env(config)\n",
    "    \n",
    "    # Create Q-networks\n",
    "    model = create_q_model(config.num_actions)\n",
    "    target_model = create_q_model(config.num_actions)\n",
    "    target_model.set_weights(model.get_weights())\n",
    "    \n",
    "    # Initialize replay memory\n",
    "    memory = ReplayMemory(config.replay_memory_size)\n",
    "    \n",
    "    # Training metrics\n",
    "    frame_count = 0\n",
    "    episode_count = 0\n",
    "    rewards_history = []\n",
    "    epsilon_history = []\n",
    "    avg_q_history = []\n",
    "    loss_history = []\n",
    "    \n",
    "    # Start training\n",
    "    print(\"Starting training...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    while frame_count < config.total_frames:\n",
    "        episode_count += 1\n",
    "        state, _ = env.reset()\n",
    "        episode_reward = 0\n",
    "        episode_loss = []\n",
    "        episode_q_values = []\n",
    "        \n",
    "        for step in range(config.max_episode_length):\n",
    "            frame_count += 1\n",
    "            epsilon = config.epsilon_by_frame(frame_count)\n",
    "            epsilon_history.append(epsilon)\n",
    "            \n",
    "            # Select action: epsilon-greedy policy\n",
    "            if random.random() < epsilon:\n",
    "                action = random.randrange(config.num_actions)\n",
    "            else:\n",
    "                state_tensor = tf.convert_to_tensor(state)\n",
    "                state_tensor = tf.expand_dims(state_tensor, 0)\n",
    "                q_values = model(state_tensor, training=False)\n",
    "                episode_q_values.append(float(tf.reduce_mean(q_values)))\n",
    "                action = tf.argmax(q_values[0]).numpy()\n",
    "            \n",
    "            # Take action and observe result\n",
    "            next_state, reward, done, truncated, _ = env.step(action)\n",
    "            episode_reward += reward\n",
    "            \n",
    "            # Store transition in memory\n",
    "            memory.add(state, action, reward, next_state, done or truncated)\n",
    "            \n",
    "            # Move to next state\n",
    "            state = next_state\n",
    "            \n",
    "            # Train every 4 frames once memory has enough samples\n",
    "            if frame_count % 4 == 0 and len(memory) > config.min_replay_memory_size:\n",
    "                # Sample from replay memory\n",
    "                states, actions, rewards, next_states, dones = memory.sample(config.batch_size)\n",
    "                \n",
    "                # Calculate target Q values\n",
    "                future_rewards = target_model.predict(next_states, verbose=0)\n",
    "                target_q_values = rewards + config.gamma * np.max(future_rewards, axis=1) * (1 - dones)\n",
    "                \n",
    "                # Update Q-value for actions taken\n",
    "                masks = tf.one_hot(actions, config.num_actions)\n",
    "                with tf.GradientTape() as tape:\n",
    "                    q_values = model(states)\n",
    "                    q_action = tf.reduce_sum(q_values * masks, axis=1)\n",
    "                    loss = tf.reduce_mean(keras.losses.huber(target_q_values, q_action))\n",
    "                \n",
    "                # Backpropagation\n",
    "                grads = tape.gradient(loss, model.trainable_variables)\n",
    "                model.optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "                episode_loss.append(float(loss))\n",
    "            \n",
    "            # Update target network\n",
    "            if frame_count % config.target_update_freq == 0:\n",
    "                target_model.set_weights(model.get_weights())\n",
    "                print(f\"Updated target network at frame {frame_count}\")\n",
    "            \n",
    "            # Save model\n",
    "            if frame_count % config.save_freq == 0:\n",
    "                model_path = os.path.join(config.checkpoint_dir, f\"model_frame_{frame_count}.keras\")\n",
    "                model.save(model_path)\n",
    "                print(f\"Model saved at {model_path}\")\n",
    "            \n",
    "            # Show progress\n",
    "            if frame_count % 10000 == 0:\n",
    "                elapsed_time = time.time() - start_time\n",
    "                avg_reward = np.mean(rewards_history[-100:]) if rewards_history else 0\n",
    "                print(f\"Frame: {frame_count}/{config.total_frames} | \"\n",
    "                      f\"Episode: {episode_count} | \"\n",
    "                      f\"Epsilon: {epsilon:.4f} | \"\n",
    "                      f\"Avg Reward (last 100): {avg_reward:.2f} | \"\n",
    "                      f\"Time elapsed: {elapsed_time:.2f}s\")\n",
    "            \n",
    "            if done or truncated:\n",
    "                break\n",
    "        \n",
    "        # Record episode statistics\n",
    "        rewards_history.append(episode_reward)\n",
    "        avg_q = np.mean(episode_q_values) if episode_q_values else 0\n",
    "        avg_q_history.append(avg_q)\n",
    "        avg_loss = np.mean(episode_loss) if episode_loss else 0\n",
    "        loss_history.append(avg_loss)\n",
    "        \n",
    "        print(f\"Episode {episode_count} finished | \"\n",
    "              f\"Frames: {frame_count} | \"\n",
    "              f\"Reward: {episode_reward} | \"\n",
    "              f\"Avg Q: {avg_q:.4f} | \"\n",
    "              f\"Avg Loss: {avg_loss:.4f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if len(rewards_history) >= 10 and episode_reward >= max(rewards_history[:-1]):\n",
    "            model_path = os.path.join(config.checkpoint_dir, \"best_model.keras\")\n",
    "            model.save(model_path)\n",
    "            print(f\"Saved best model with reward {episode_reward}\")\n",
    "    \n",
    "    # Save final model\n",
    "    final_model_path = os.path.join(config.checkpoint_dir, \"final_model.keras\")\n",
    "    model.save(final_model_path)\n",
    "    print(f\"Training completed. Final model saved at {final_model_path}\")\n",
    "    \n",
    "    return model, {\n",
    "        \"rewards\": rewards_history,\n",
    "        \"epsilon\": epsilon_history,\n",
    "        \"avg_q\": avg_q_history,\n",
    "        \"loss\": loss_history\n",
    "    }\n",
    "\n",
    "# Function to evaluate trained model\n",
    "def evaluate_model(model_path, num_episodes=10):\n",
    "    config = DQNConfig()\n",
    "    config.render_mode = \"human\"  # Set to human to visualize\n",
    "    \n",
    "    env = create_env(config)\n",
    "    model = keras.models.load_model(model_path)\n",
    "    \n",
    "    total_rewards = []\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        \n",
    "        while not done:\n",
    "            state_tensor = tf.convert_to_tensor(state)\n",
    "            state_tensor = tf.expand_dims(state_tensor, 0)\n",
    "            action_probs = model(state_tensor, training=False)\n",
    "            action = tf.argmax(action_probs[0]).numpy()\n",
    "            \n",
    "            state, reward, done, truncated, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            \n",
    "            if done or truncated:\n",
    "                break\n",
    "        \n",
    "        total_rewards.append(total_reward)\n",
    "        print(f\"Episode {episode+1}: Reward = {total_reward}\")\n",
    "    \n",
    "    print(f\"Average reward over {num_episodes} episodes: {np.mean(total_rewards)}\")\n",
    "    env.close()\n",
    "\n",
    "# Run training if executed directly\n",
    "if __name__ == \"__main__\":\n",
    "    model, metrics = train_dqn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLVenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
