{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Episode 1 finished | Frames: 131 | Reward: 15.0 | Avg Q: 0.0000 | Avg Loss: 0.0000\n",
      "Episode 2 finished | Frames: 334 | Reward: 80.0 | Avg Q: 0.0000 | Avg Loss: 0.0000\n",
      "Episode 3 finished | Frames: 487 | Reward: 5.0 | Avg Q: 0.0000 | Avg Loss: 0.0000\n",
      "Episode 4 finished | Frames: 696 | Reward: 75.0 | Avg Q: 0.0000 | Avg Loss: 0.0000\n",
      "Episode 5 finished | Frames: 853 | Reward: 40.0 | Avg Q: -1.5749 | Avg Loss: 0.0000\n",
      "Episode 6 finished | Frames: 1037 | Reward: 45.0 | Avg Q: -1.7451 | Avg Loss: 0.0000\n",
      "Episode 7 finished | Frames: 1182 | Reward: 40.0 | Avg Q: 0.0000 | Avg Loss: 0.0000\n",
      "Episode 8 finished | Frames: 1368 | Reward: 20.0 | Avg Q: -0.9766 | Avg Loss: 0.0000\n",
      "Episode 9 finished | Frames: 1553 | Reward: 5.0 | Avg Q: -1.9736 | Avg Loss: 0.0000\n",
      "Episode 10 finished | Frames: 1888 | Reward: 90.0 | Avg Q: -1.0486 | Avg Loss: 0.0000\n",
      "Saved best model with reward 90.0\n",
      "Episode 11 finished | Frames: 2176 | Reward: 60.0 | Avg Q: -1.5339 | Avg Loss: 0.0000\n",
      "Episode 12 finished | Frames: 2381 | Reward: 105.0 | Avg Q: -1.4601 | Avg Loss: 0.0000\n",
      "Saved best model with reward 105.0\n",
      "Episode 13 finished | Frames: 2527 | Reward: 20.0 | Avg Q: -1.6699 | Avg Loss: 0.0000\n",
      "Episode 14 finished | Frames: 2639 | Reward: 5.0 | Avg Q: -1.3098 | Avg Loss: 0.0000\n",
      "Episode 15 finished | Frames: 2921 | Reward: 40.0 | Avg Q: -1.9167 | Avg Loss: 0.0000\n",
      "Episode 16 finished | Frames: 3070 | Reward: 5.0 | Avg Q: -1.9042 | Avg Loss: 0.0000\n",
      "Episode 17 finished | Frames: 3225 | Reward: 30.0 | Avg Q: -1.6959 | Avg Loss: 0.0000\n",
      "Episode 18 finished | Frames: 3387 | Reward: 30.0 | Avg Q: -1.4966 | Avg Loss: 0.0000\n",
      "Episode 19 finished | Frames: 3541 | Reward: 5.0 | Avg Q: -1.5297 | Avg Loss: 0.0000\n",
      "Episode 20 finished | Frames: 3662 | Reward: 0.0 | Avg Q: -1.0859 | Avg Loss: 0.0000\n",
      "Episode 21 finished | Frames: 3848 | Reward: 50.0 | Avg Q: -1.8650 | Avg Loss: 0.0000\n",
      "Episode 22 finished | Frames: 4188 | Reward: 120.0 | Avg Q: -1.2030 | Avg Loss: 0.0000\n",
      "Saved best model with reward 120.0\n",
      "Episode 23 finished | Frames: 4347 | Reward: 30.0 | Avg Q: -1.9447 | Avg Loss: 0.0000\n",
      "Episode 24 finished | Frames: 4555 | Reward: 35.0 | Avg Q: -1.6932 | Avg Loss: 0.0000\n",
      "Episode 25 finished | Frames: 4712 | Reward: 75.0 | Avg Q: -1.0471 | Avg Loss: 0.0000\n",
      "Episode 26 finished | Frames: 4873 | Reward: 60.0 | Avg Q: -1.6444 | Avg Loss: 0.0000\n",
      "Episode 27 finished | Frames: 5008 | Reward: 15.0 | Avg Q: -2.0338 | Avg Loss: 0.0000\n",
      "Episode 28 finished | Frames: 5198 | Reward: 60.0 | Avg Q: -1.4320 | Avg Loss: 0.0000\n",
      "Episode 29 finished | Frames: 5609 | Reward: 350.0 | Avg Q: -0.8059 | Avg Loss: 0.0000\n",
      "Saved best model with reward 350.0\n",
      "Episode 30 finished | Frames: 5792 | Reward: 55.0 | Avg Q: -2.1015 | Avg Loss: 0.0000\n",
      "Episode 31 finished | Frames: 6068 | Reward: 145.0 | Avg Q: -1.5765 | Avg Loss: 0.0000\n",
      "Episode 32 finished | Frames: 6288 | Reward: 35.0 | Avg Q: -1.4696 | Avg Loss: 0.0000\n",
      "Episode 33 finished | Frames: 6446 | Reward: 50.0 | Avg Q: -1.5662 | Avg Loss: 0.0000\n",
      "Episode 34 finished | Frames: 6591 | Reward: 15.0 | Avg Q: -1.9732 | Avg Loss: 0.0000\n",
      "Episode 35 finished | Frames: 6797 | Reward: 50.0 | Avg Q: -1.8768 | Avg Loss: 0.0000\n",
      "Episode 36 finished | Frames: 6949 | Reward: 30.0 | Avg Q: -1.2753 | Avg Loss: 0.0000\n",
      "Episode 37 finished | Frames: 7153 | Reward: 60.0 | Avg Q: -1.4038 | Avg Loss: 0.0000\n",
      "Episode 38 finished | Frames: 7309 | Reward: 30.0 | Avg Q: -1.5167 | Avg Loss: 0.0000\n",
      "Episode 39 finished | Frames: 7516 | Reward: 105.0 | Avg Q: -1.7182 | Avg Loss: 0.0000\n",
      "Episode 40 finished | Frames: 7711 | Reward: 75.0 | Avg Q: -1.6939 | Avg Loss: 0.0000\n",
      "Episode 41 finished | Frames: 7855 | Reward: 15.0 | Avg Q: -1.6663 | Avg Loss: 0.0000\n",
      "Episode 42 finished | Frames: 8012 | Reward: 75.0 | Avg Q: -1.7293 | Avg Loss: 0.0000\n",
      "Episode 43 finished | Frames: 8114 | Reward: 0.0 | Avg Q: -1.5661 | Avg Loss: 0.0000\n",
      "Episode 44 finished | Frames: 8296 | Reward: 60.0 | Avg Q: -1.5119 | Avg Loss: 0.0000\n",
      "Episode 45 finished | Frames: 8512 | Reward: 35.0 | Avg Q: -1.9305 | Avg Loss: 0.0000\n",
      "Episode 46 finished | Frames: 8742 | Reward: 65.0 | Avg Q: -1.2807 | Avg Loss: 0.0000\n",
      "Episode 47 finished | Frames: 8893 | Reward: 75.0 | Avg Q: -1.3631 | Avg Loss: 0.0000\n",
      "Episode 48 finished | Frames: 9030 | Reward: 50.0 | Avg Q: -1.8320 | Avg Loss: 0.0000\n",
      "Episode 49 finished | Frames: 9180 | Reward: 30.0 | Avg Q: -1.1320 | Avg Loss: 0.0000\n",
      "Episode 50 finished | Frames: 9408 | Reward: 75.0 | Avg Q: -1.4943 | Avg Loss: 0.0000\n",
      "Episode 51 finished | Frames: 9836 | Reward: 155.0 | Avg Q: -0.9378 | Avg Loss: 0.0000\n",
      "Updated target network at frame 10000\n",
      "Frame: 10000/5000000 | Episode: 52 | Epsilon: 0.8200 | Avg Reward (last 100): 54.80 | Time elapsed: 16.27s\n",
      "Episode 52 finished | Frames: 10053 | Reward: 10.0 | Avg Q: -1.0688 | Avg Loss: 2.5151\n",
      "Episode 53 finished | Frames: 10218 | Reward: 60.0 | Avg Q: 4.3766 | Avg Loss: 0.9806\n",
      "Episode 54 finished | Frames: 10417 | Reward: 35.0 | Avg Q: 3.4950 | Avg Loss: 0.7406\n",
      "Episode 55 finished | Frames: 10797 | Reward: 280.0 | Avg Q: 4.1884 | Avg Loss: 0.6202\n",
      "Episode 56 finished | Frames: 11023 | Reward: 55.0 | Avg Q: 3.9341 | Avg Loss: 0.7496\n",
      "Episode 57 finished | Frames: 11226 | Reward: 75.0 | Avg Q: 3.6303 | Avg Loss: 0.6559\n",
      "Episode 58 finished | Frames: 11426 | Reward: 45.0 | Avg Q: 3.3591 | Avg Loss: 0.5265\n",
      "Episode 59 finished | Frames: 11675 | Reward: 110.0 | Avg Q: 4.1977 | Avg Loss: 0.6720\n",
      "Episode 60 finished | Frames: 11814 | Reward: 15.0 | Avg Q: 3.5318 | Avg Loss: 0.5387\n",
      "Episode 61 finished | Frames: 11993 | Reward: 45.0 | Avg Q: 4.4121 | Avg Loss: 0.4459\n",
      "Episode 62 finished | Frames: 12171 | Reward: 20.0 | Avg Q: 3.6050 | Avg Loss: 0.4986\n",
      "Episode 63 finished | Frames: 12427 | Reward: 90.0 | Avg Q: 3.9351 | Avg Loss: 0.4535\n",
      "Episode 64 finished | Frames: 12567 | Reward: 0.0 | Avg Q: 3.1444 | Avg Loss: 0.4945\n",
      "Episode 65 finished | Frames: 12855 | Reward: 85.0 | Avg Q: 4.3364 | Avg Loss: 0.5424\n",
      "Episode 66 finished | Frames: 13013 | Reward: 40.0 | Avg Q: 4.3921 | Avg Loss: 0.5914\n",
      "Episode 67 finished | Frames: 13201 | Reward: 40.0 | Avg Q: 4.1374 | Avg Loss: 0.5119\n",
      "Episode 68 finished | Frames: 13291 | Reward: 0.0 | Avg Q: 3.4471 | Avg Loss: 0.4066\n",
      "Episode 69 finished | Frames: 13598 | Reward: 85.0 | Avg Q: 4.1691 | Avg Loss: 0.3988\n",
      "Episode 70 finished | Frames: 13742 | Reward: 15.0 | Avg Q: 3.7569 | Avg Loss: 0.4199\n",
      "Episode 71 finished | Frames: 13892 | Reward: 5.0 | Avg Q: 4.0124 | Avg Loss: 0.5214\n",
      "Episode 72 finished | Frames: 14033 | Reward: 30.0 | Avg Q: 3.7598 | Avg Loss: 0.3439\n",
      "Episode 73 finished | Frames: 14249 | Reward: 55.0 | Avg Q: 4.0176 | Avg Loss: 0.3825\n",
      "Episode 74 finished | Frames: 14401 | Reward: 50.0 | Avg Q: 4.4920 | Avg Loss: 0.3793\n",
      "Episode 75 finished | Frames: 14585 | Reward: 30.0 | Avg Q: 4.0500 | Avg Loss: 0.5403\n",
      "Episode 76 finished | Frames: 14679 | Reward: 0.0 | Avg Q: 2.9923 | Avg Loss: 0.3838\n",
      "Episode 77 finished | Frames: 14836 | Reward: 45.0 | Avg Q: 4.7994 | Avg Loss: 0.5945\n",
      "Episode 78 finished | Frames: 15039 | Reward: 50.0 | Avg Q: 3.7413 | Avg Loss: 0.6008\n",
      "Episode 79 finished | Frames: 15192 | Reward: 50.0 | Avg Q: 4.4288 | Avg Loss: 0.4214\n",
      "Episode 80 finished | Frames: 15467 | Reward: 40.0 | Avg Q: 3.4900 | Avg Loss: 0.5893\n",
      "Episode 81 finished | Frames: 15641 | Reward: 30.0 | Avg Q: 4.6169 | Avg Loss: 0.5692\n",
      "Episode 82 finished | Frames: 15789 | Reward: 5.0 | Avg Q: 3.8138 | Avg Loss: 0.6768\n",
      "Episode 83 finished | Frames: 15931 | Reward: 30.0 | Avg Q: 3.8128 | Avg Loss: 0.3613\n",
      "Episode 84 finished | Frames: 16226 | Reward: 70.0 | Avg Q: 3.8180 | Avg Loss: 0.3273\n",
      "Episode 85 finished | Frames: 16310 | Reward: 0.0 | Avg Q: 3.7589 | Avg Loss: 0.6971\n",
      "Episode 86 finished | Frames: 16512 | Reward: 45.0 | Avg Q: 3.6567 | Avg Loss: 0.3829\n",
      "Episode 87 finished | Frames: 16818 | Reward: 65.0 | Avg Q: 4.1422 | Avg Loss: 0.4915\n",
      "Episode 88 finished | Frames: 16972 | Reward: 45.0 | Avg Q: 4.3731 | Avg Loss: 0.5362\n",
      "Episode 89 finished | Frames: 17095 | Reward: 30.0 | Avg Q: 3.5524 | Avg Loss: 0.5604\n",
      "Episode 90 finished | Frames: 17401 | Reward: 75.0 | Avg Q: 4.0962 | Avg Loss: 0.3338\n",
      "Episode 91 finished | Frames: 17556 | Reward: 20.0 | Avg Q: 4.6488 | Avg Loss: 0.4437\n",
      "Episode 92 finished | Frames: 17667 | Reward: 5.0 | Avg Q: 3.4728 | Avg Loss: 0.3647\n",
      "Episode 93 finished | Frames: 17808 | Reward: 55.0 | Avg Q: 4.0676 | Avg Loss: 0.6373\n",
      "Episode 94 finished | Frames: 17978 | Reward: 30.0 | Avg Q: 4.2351 | Avg Loss: 0.3980\n",
      "Episode 95 finished | Frames: 18147 | Reward: 75.0 | Avg Q: 3.9367 | Avg Loss: 0.3667\n",
      "Episode 96 finished | Frames: 18301 | Reward: 60.0 | Avg Q: 4.4593 | Avg Loss: 0.2990\n",
      "Episode 97 finished | Frames: 18537 | Reward: 35.0 | Avg Q: 3.9486 | Avg Loss: 0.4377\n",
      "Episode 98 finished | Frames: 18684 | Reward: 30.0 | Avg Q: 3.3870 | Avg Loss: 0.3772\n",
      "Episode 99 finished | Frames: 18871 | Reward: 60.0 | Avg Q: 3.2755 | Avg Loss: 0.3580\n",
      "Episode 100 finished | Frames: 19054 | Reward: 50.0 | Avg Q: 4.7650 | Avg Loss: 0.3479\n",
      "Episode 101 finished | Frames: 19139 | Reward: 5.0 | Avg Q: 3.7473 | Avg Loss: 0.3960\n",
      "Updated target network at frame 20000\n",
      "Frame: 20000/5000000 | Episode: 102 | Epsilon: 0.6400 | Avg Reward (last 100): 50.60 | Time elapsed: 457.88s\n",
      "Episode 102 finished | Frames: 20209 | Reward: 660.0 | Avg Q: 3.5021 | Avg Loss: 0.4027\n",
      "Saved best model with reward 660.0\n",
      "Episode 103 finished | Frames: 20439 | Reward: 45.0 | Avg Q: 3.8357 | Avg Loss: 0.5161\n",
      "Episode 104 finished | Frames: 20743 | Reward: 60.0 | Avg Q: 4.5783 | Avg Loss: 0.4634\n",
      "Episode 105 finished | Frames: 20928 | Reward: 55.0 | Avg Q: 5.0762 | Avg Loss: 0.3426\n",
      "Episode 106 finished | Frames: 21120 | Reward: 80.0 | Avg Q: 4.7145 | Avg Loss: 0.5041\n",
      "Episode 107 finished | Frames: 21294 | Reward: 30.0 | Avg Q: 5.6916 | Avg Loss: 0.5113\n",
      "Episode 108 finished | Frames: 21447 | Reward: 50.0 | Avg Q: 4.7003 | Avg Loss: 0.3227\n",
      "Episode 109 finished | Frames: 21652 | Reward: 50.0 | Avg Q: 3.3098 | Avg Loss: 0.4301\n",
      "Episode 110 finished | Frames: 21808 | Reward: 10.0 | Avg Q: 4.5545 | Avg Loss: 0.3168\n",
      "Episode 111 finished | Frames: 21932 | Reward: 5.0 | Avg Q: 3.7160 | Avg Loss: 0.3066\n",
      "Episode 112 finished | Frames: 22082 | Reward: 15.0 | Avg Q: 4.9357 | Avg Loss: 0.3105\n",
      "Episode 113 finished | Frames: 22386 | Reward: 125.0 | Avg Q: 4.2979 | Avg Loss: 0.3141\n",
      "Episode 114 finished | Frames: 22533 | Reward: 45.0 | Avg Q: 4.6702 | Avg Loss: 0.4227\n",
      "Episode 115 finished | Frames: 22675 | Reward: 0.0 | Avg Q: 4.0577 | Avg Loss: 0.3339\n",
      "Episode 116 finished | Frames: 22856 | Reward: 30.0 | Avg Q: 4.7696 | Avg Loss: 0.2576\n",
      "Episode 117 finished | Frames: 23001 | Reward: 15.0 | Avg Q: 4.6235 | Avg Loss: 0.3178\n",
      "Episode 118 finished | Frames: 23195 | Reward: 75.0 | Avg Q: 3.9637 | Avg Loss: 0.3848\n",
      "Episode 119 finished | Frames: 23310 | Reward: 5.0 | Avg Q: 3.9334 | Avg Loss: 0.2255\n",
      "Episode 120 finished | Frames: 23494 | Reward: 25.0 | Avg Q: 4.7687 | Avg Loss: 0.4051\n",
      "Episode 121 finished | Frames: 23641 | Reward: 40.0 | Avg Q: 4.6883 | Avg Loss: 0.3783\n",
      "Episode 122 finished | Frames: 23779 | Reward: 15.0 | Avg Q: 3.8863 | Avg Loss: 0.3429\n",
      "Episode 123 finished | Frames: 23924 | Reward: 50.0 | Avg Q: 3.8340 | Avg Loss: 0.3339\n",
      "Episode 124 finished | Frames: 24106 | Reward: 105.0 | Avg Q: 5.2165 | Avg Loss: 0.2622\n",
      "Episode 125 finished | Frames: 24294 | Reward: 55.0 | Avg Q: 4.3017 | Avg Loss: 0.3825\n",
      "Episode 126 finished | Frames: 24480 | Reward: 35.0 | Avg Q: 5.2952 | Avg Loss: 0.3356\n",
      "Episode 127 finished | Frames: 24594 | Reward: 15.0 | Avg Q: 4.0503 | Avg Loss: 0.3252\n",
      "Episode 128 finished | Frames: 24779 | Reward: 30.0 | Avg Q: 4.3725 | Avg Loss: 0.4006\n",
      "Episode 129 finished | Frames: 24931 | Reward: 50.0 | Avg Q: 4.7205 | Avg Loss: 0.4148\n",
      "Episode 130 finished | Frames: 25118 | Reward: 50.0 | Avg Q: 4.9560 | Avg Loss: 0.3096\n",
      "Episode 131 finished | Frames: 25280 | Reward: 70.0 | Avg Q: 4.7949 | Avg Loss: 0.3729\n",
      "Episode 132 finished | Frames: 25470 | Reward: 35.0 | Avg Q: 3.7188 | Avg Loss: 0.3151\n",
      "Episode 133 finished | Frames: 25618 | Reward: 15.0 | Avg Q: 5.4604 | Avg Loss: 0.3679\n",
      "Episode 134 finished | Frames: 25978 | Reward: 80.0 | Avg Q: 5.2257 | Avg Loss: 0.4284\n",
      "Episode 135 finished | Frames: 26161 | Reward: 0.0 | Avg Q: 4.5044 | Avg Loss: 0.3463\n",
      "Episode 136 finished | Frames: 26313 | Reward: 15.0 | Avg Q: 5.6640 | Avg Loss: 0.4837\n",
      "Episode 137 finished | Frames: 26438 | Reward: 25.0 | Avg Q: 3.6359 | Avg Loss: 0.2732\n",
      "Episode 138 finished | Frames: 26638 | Reward: 30.0 | Avg Q: 3.8457 | Avg Loss: 0.2856\n",
      "Episode 139 finished | Frames: 26811 | Reward: 30.0 | Avg Q: 5.6234 | Avg Loss: 0.3357\n",
      "Episode 140 finished | Frames: 26967 | Reward: 15.0 | Avg Q: 4.3464 | Avg Loss: 0.3326\n",
      "Episode 141 finished | Frames: 27138 | Reward: 5.0 | Avg Q: 4.4402 | Avg Loss: 0.3673\n",
      "Episode 142 finished | Frames: 27449 | Reward: 45.0 | Avg Q: 4.7176 | Avg Loss: 0.2163\n",
      "Episode 143 finished | Frames: 27857 | Reward: 380.0 | Avg Q: 4.5769 | Avg Loss: 0.3243\n",
      "Episode 144 finished | Frames: 28122 | Reward: 90.0 | Avg Q: 4.6241 | Avg Loss: 0.4060\n",
      "Episode 145 finished | Frames: 28315 | Reward: 50.0 | Avg Q: 4.0558 | Avg Loss: 0.4165\n",
      "Episode 146 finished | Frames: 28509 | Reward: 60.0 | Avg Q: 4.0134 | Avg Loss: 0.3899\n",
      "Episode 147 finished | Frames: 28709 | Reward: 75.0 | Avg Q: 3.4259 | Avg Loss: 0.2955\n",
      "Episode 148 finished | Frames: 29258 | Reward: 170.0 | Avg Q: 4.1528 | Avg Loss: 0.3477\n",
      "Episode 149 finished | Frames: 29409 | Reward: 45.0 | Avg Q: 5.1724 | Avg Loss: 0.2938\n",
      "Episode 150 finished | Frames: 29784 | Reward: 145.0 | Avg Q: 4.9665 | Avg Loss: 0.3151\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import gymnasium as gym\n",
    "import ale_py\n",
    "from collections import deque\n",
    "import time\n",
    "\n",
    "# Register Atari environments\n",
    "gym.register_envs(ale_py)\n",
    "\n",
    "# Custom frame stacking to replace gymnasium's FrameStack\n",
    "class CustomFrameStack:\n",
    "    def __init__(self, env, num_stack=4):\n",
    "        self.env = env\n",
    "        self.num_stack = num_stack\n",
    "        self.frames = deque(maxlen=num_stack)\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=0, high=255, \n",
    "            shape=(84, 84, num_stack), \n",
    "            dtype=np.uint8\n",
    "        )\n",
    "        self.action_space = env.action_space\n",
    "        \n",
    "    def reset(self, **kwargs):\n",
    "        obs, info = self.env.reset(**kwargs)\n",
    "        for _ in range(self.num_stack):\n",
    "            self.frames.append(obs)\n",
    "        stacked_frames = np.stack(self.frames, axis=-1)\n",
    "        return stacked_frames, info\n",
    "    \n",
    "    def step(self, action):\n",
    "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "        self.frames.append(obs)\n",
    "        stacked_frames = np.stack(self.frames, axis=-1)\n",
    "        return stacked_frames, reward, terminated, truncated, info\n",
    "    \n",
    "    def close(self):\n",
    "        return self.env.close()\n",
    "        \n",
    "    def render(self):\n",
    "        return self.env.render()\n",
    "\n",
    "# Configuration parameters\n",
    "class DQNConfig:\n",
    "    def __init__(self):\n",
    "        # Environment\n",
    "        self.env_name = \"SpaceInvadersNoFrameskip-v4\"\n",
    "        self.render_mode = None  # Set to \"human\" to visualize\n",
    "        self.num_stack = 4\n",
    "        \n",
    "        # Training hyperparameters\n",
    "        self.gamma = 0.99  # Discount factor\n",
    "        self.learning_rate = 0.00025\n",
    "        self.batch_size = 32\n",
    "        self.epsilon_start = 1.0\n",
    "        self.epsilon_min = 0.1\n",
    "        self.epsilon_decay_frames = 50000  # Frames over which to decay epsilon\n",
    "        self.target_update_freq = 10000  # Update target network every N steps\n",
    "        \n",
    "        # Memory\n",
    "        self.replay_memory_size = 100000  # Reduced from original paper to save memory\n",
    "        self.min_replay_memory_size = 10000  # Start training after this many frames\n",
    "        \n",
    "        # Training limits\n",
    "        self.total_frames = 5000000  # Total frames to train for (reduced from paper's 50M)\n",
    "        self.max_episode_length = 10000  # Max frames per episode\n",
    "        \n",
    "        # Checkpointing\n",
    "        self.save_freq = 50000  # Save model every N frames\n",
    "        self.checkpoint_dir = \"checkpoints\"\n",
    "        \n",
    "        # Game specific\n",
    "        self.num_actions = 6  # Space Invaders has 6 actions\n",
    "        \n",
    "    def epsilon_by_frame(self, frame):\n",
    "        # Linear epsilon decay\n",
    "        epsilon = self.epsilon_start - frame * (self.epsilon_start - self.epsilon_min) / self.epsilon_decay_frames\n",
    "        return max(self.epsilon_min, epsilon)\n",
    "\n",
    "# Deep Q-Network architecture\n",
    "def create_q_model(num_actions):\n",
    "    model = keras.Sequential([\n",
    "        layers.Input(shape=(84, 84, 4)),\n",
    "        layers.Conv2D(32, kernel_size=8, strides=4, activation=\"relu\"),\n",
    "        layers.Conv2D(64, kernel_size=4, strides=2, activation=\"relu\"),\n",
    "        layers.Conv2D(64, kernel_size=3, strides=1, activation=\"relu\"),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(512, activation=\"relu\"),\n",
    "        layers.Dense(num_actions, activation=\"linear\")\n",
    "    ])\n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.00025, clipnorm=1.0),\n",
    "                  loss=keras.losses.Huber())\n",
    "    return model\n",
    "\n",
    "# Create environment with preprocessing\n",
    "def create_env(config):\n",
    "    env = gym.make(config.env_name, render_mode=config.render_mode)\n",
    "    # Apply Atari preprocessing: max pooling, frame skipping, etc.\n",
    "    env = gym.wrappers.AtariPreprocessing(\n",
    "        env,\n",
    "        frame_skip=4,\n",
    "        screen_size=84,\n",
    "        grayscale_obs=True,\n",
    "        scale_obs=False,\n",
    "        terminal_on_life_loss=True  # This helps with training\n",
    "    )\n",
    "    # Use our custom frame stacking\n",
    "    env = CustomFrameStack(env, config.num_stack)\n",
    "    return env\n",
    "\n",
    "# Experience replay memory\n",
    "class ReplayMemory:\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque(maxlen=capacity)\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.memory, batch_size)\n",
    "        states, actions, rewards, next_states, dones = map(np.array, zip(*batch))\n",
    "        return states, actions, rewards, next_states, dones\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "# Main training function\n",
    "def train_dqn():\n",
    "    # Initialize configuration\n",
    "    config = DQNConfig()\n",
    "    \n",
    "    # Create checkpoint directory\n",
    "    os.makedirs(config.checkpoint_dir, exist_ok=True)\n",
    "    \n",
    "    # Set up environment\n",
    "    env = create_env(config)\n",
    "    \n",
    "    # Create Q-networks\n",
    "    model = create_q_model(config.num_actions)\n",
    "    target_model = create_q_model(config.num_actions)\n",
    "    target_model.set_weights(model.get_weights())\n",
    "    \n",
    "    # Initialize replay memory\n",
    "    memory = ReplayMemory(config.replay_memory_size)\n",
    "    \n",
    "    # Training metrics\n",
    "    frame_count = 0\n",
    "    episode_count = 0\n",
    "    rewards_history = []\n",
    "    epsilon_history = []\n",
    "    avg_q_history = []\n",
    "    loss_history = []\n",
    "    \n",
    "    # Start training\n",
    "    print(\"Starting training...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    while frame_count < config.total_frames:\n",
    "        episode_count += 1\n",
    "        state, _ = env.reset()\n",
    "        episode_reward = 0\n",
    "        episode_loss = []\n",
    "        episode_q_values = []\n",
    "        \n",
    "        for step in range(config.max_episode_length):\n",
    "            frame_count += 1\n",
    "            epsilon = config.epsilon_by_frame(frame_count)\n",
    "            epsilon_history.append(epsilon)\n",
    "            \n",
    "            # Select action: epsilon-greedy policy\n",
    "            if random.random() < epsilon:\n",
    "                action = random.randrange(config.num_actions)\n",
    "            else:\n",
    "                state_tensor = tf.convert_to_tensor(state)\n",
    "                state_tensor = tf.expand_dims(state_tensor, 0)\n",
    "                q_values = model(state_tensor, training=False)\n",
    "                episode_q_values.append(float(tf.reduce_mean(q_values)))\n",
    "                action = tf.argmax(q_values[0]).numpy()\n",
    "            \n",
    "            # Take action and observe result\n",
    "            next_state, reward, done, truncated, _ = env.step(action)\n",
    "            episode_reward += reward\n",
    "            \n",
    "            # Store transition in memory\n",
    "            memory.add(state, action, reward, next_state, done or truncated)\n",
    "            \n",
    "            # Move to next state\n",
    "            state = next_state\n",
    "            \n",
    "            # Train every 4 frames once memory has enough samples\n",
    "            if frame_count % 4 == 0 and len(memory) > config.min_replay_memory_size:\n",
    "                # Sample from replay memory\n",
    "                states, actions, rewards, next_states, dones = memory.sample(config.batch_size)\n",
    "                \n",
    "                # Calculate target Q values\n",
    "                future_rewards = target_model.predict(next_states, verbose=0)\n",
    "                target_q_values = rewards + config.gamma * np.max(future_rewards, axis=1) * (1 - dones)\n",
    "                \n",
    "                # Update Q-value for actions taken\n",
    "                masks = tf.one_hot(actions, config.num_actions)\n",
    "                with tf.GradientTape() as tape:\n",
    "                    q_values = model(states)\n",
    "                    q_action = tf.reduce_sum(q_values * masks, axis=1)\n",
    "                    loss = tf.reduce_mean(keras.losses.huber(target_q_values, q_action))\n",
    "                \n",
    "                # Backpropagation\n",
    "                grads = tape.gradient(loss, model.trainable_variables)\n",
    "                model.optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "                episode_loss.append(float(loss))\n",
    "            \n",
    "            # Update target network\n",
    "            if frame_count % config.target_update_freq == 0:\n",
    "                target_model.set_weights(model.get_weights())\n",
    "                print(f\"Updated target network at frame {frame_count}\")\n",
    "            \n",
    "            # Save model\n",
    "            if frame_count % config.save_freq == 0:\n",
    "                model_path = os.path.join(config.checkpoint_dir, f\"model_frame_{frame_count}.keras\")\n",
    "                model.save(model_path)\n",
    "                print(f\"Model saved at {model_path}\")\n",
    "            \n",
    "            # Show progress\n",
    "            if frame_count % 10000 == 0:\n",
    "                elapsed_time = time.time() - start_time\n",
    "                avg_reward = np.mean(rewards_history[-100:]) if rewards_history else 0\n",
    "                print(f\"Frame: {frame_count}/{config.total_frames} | \"\n",
    "                      f\"Episode: {episode_count} | \"\n",
    "                      f\"Epsilon: {epsilon:.4f} | \"\n",
    "                      f\"Avg Reward (last 100): {avg_reward:.2f} | \"\n",
    "                      f\"Time elapsed: {elapsed_time:.2f}s\")\n",
    "            \n",
    "            if done or truncated:\n",
    "                break\n",
    "        \n",
    "        # Record episode statistics\n",
    "        rewards_history.append(episode_reward)\n",
    "        avg_q = np.mean(episode_q_values) if episode_q_values else 0\n",
    "        avg_q_history.append(avg_q)\n",
    "        avg_loss = np.mean(episode_loss) if episode_loss else 0\n",
    "        loss_history.append(avg_loss)\n",
    "        \n",
    "        print(f\"Episode {episode_count} finished | \"\n",
    "              f\"Frames: {frame_count} | \"\n",
    "              f\"Reward: {episode_reward} | \"\n",
    "              f\"Avg Q: {avg_q:.4f} | \"\n",
    "              f\"Avg Loss: {avg_loss:.4f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if len(rewards_history) >= 10 and episode_reward >= max(rewards_history[:-1]):\n",
    "            model_path = os.path.join(config.checkpoint_dir, \"best_model.keras\")\n",
    "            model.save(model_path)\n",
    "            print(f\"Saved best model with reward {episode_reward}\")\n",
    "    \n",
    "    # Save final model\n",
    "    final_model_path = os.path.join(config.checkpoint_dir, \"final_model.keras\")\n",
    "    model.save(final_model_path)\n",
    "    print(f\"Training completed. Final model saved at {final_model_path}\")\n",
    "    \n",
    "    return model, {\n",
    "        \"rewards\": rewards_history,\n",
    "        \"epsilon\": epsilon_history,\n",
    "        \"avg_q\": avg_q_history,\n",
    "        \"loss\": loss_history\n",
    "    }\n",
    "\n",
    "# Function to evaluate trained model\n",
    "def evaluate_model(model_path, num_episodes=10):\n",
    "    config = DQNConfig()\n",
    "    config.render_mode = \"human\"  # Set to human to visualize\n",
    "    \n",
    "    env = create_env(config)\n",
    "    \n",
    "    total_rewards = []\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        \n",
    "        while not done:\n",
    "            state_tensor = tf.convert_to_tensor(state)\n",
    "            state_tensor = tf.expand_dims(state_tensor, 0)\n",
    "            action_probs = model(state_tensor, training=False)\n",
    "            action = tf.argmax(action_probs[0]).numpy()\n",
    "            \n",
    "            state, reward, done, truncated, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            \n",
    "            if done or truncated:\n",
    "                break\n",
    "        \n",
    "        total_rewards.append(total_reward)\n",
    "        print(f\"Episode {episode+1}: Reward = {total_reward}\")\n",
    "    \n",
    "    print(f\"Average reward over {num_episodes} episodes: {np.mean(total_rewards)}\")\n",
    "    env.close()\n",
    "\n",
    "# Run training if executed directly\n",
    "if __name__ == \"__main__\":\n",
    "    model, metrics = train_dqn()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
