# Week 20 - Resources

[:house: Main page](https://github.com/kokchun/Deep-learning-AI21)

## Video guides :video_camera:
- [Transformers - Phi M (2020) youtube](https://www.youtube.com/watch?v=4Bdc55j80l8)

## Lecture notes :book:
- [Transformers](https://github.com/kokchun/Deep-learning-AI21/blob/main/Lectures/Lec8-Transformers.ipynb)

## Theory :book:
- [K, Q, V in attention - CrossValidated forum discussions](https://stats.stackexchange.com/questions/421935/what-exactly-are-keys-queries-and-values-in-attention-mechanisms)
- [Positional encoding in Transformers - Brownlee (2022) machinelearningmastery blog post](https://machinelearningmastery.com/a-gentle-introduction-to-positional-encoding-in-transformer-models-part-1/)
- [Positional encoding - Kazemnejad (2019) blog post](https://kazemnejad.com/blog/transformer_architecture_positional_encoding/)
- [spaCy](https://spacy.io/usage/models)
- [Hugging Face](https://huggingface.co/)
- [swedish-gpt - birgermoell Hugging Face](https://huggingface.co/birgermoell/swedish-gpt?text=grattis+p%C3%A5+f%C3%B6delsedagen)
- [GPT-2 - OpenAI team Hugging Face](https://huggingface.co/gpt2)
language_models_are_unsupervised_multitask_learners.pdf)
- [GPT-2 - wikipedia](https://en.wikipedia.org/wiki/GPT-2)
- [Named Entity Recognition (NER) - wikipedia](https://en.wikipedia.org/wiki/Named-entity_recognition)
- [spaCy English](https://spacy.io/models/en)

Research articles
- [Attention is all you need - Vaswani et. al. (2017)](https://arxiv.org/pdf/1706.03762.pdf)
- [BERT: Bidirectional Encoder Representation from Transformers - Devlin et. al. (2019)](https://arxiv.org/pdf/1810.04805.pdf)
- [GPT-2 paper - Radford et. al. (2018)](https://d4mucfpksywv.cloudfront.net/better-language-models/)

## Exercises :running:
Work with the project
